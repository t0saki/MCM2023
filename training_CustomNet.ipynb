{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer\n",
    "from models.CustomNet import CustomNet\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_word_list(word_list, max_len=5):\n",
    "    \"\"\"Resize a list of words into a fixed length, truncating or padding as necessary\"\"\"\n",
    "    resized_list = []\n",
    "    for word in word_list:\n",
    "        # Truncate or pad word to fixed length\n",
    "        if len(word) > max_len:\n",
    "            resized_word = word[:max_len]\n",
    "        else:\n",
    "            resized_word = word + '0'*(max_len-len(word))\n",
    "        resized_list.append(resized_word)\n",
    "    return resized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset class\n",
    "word_column = 'Word'\n",
    "input_columns = ['Month', 'Day', 'WeekNum', 'Contest number']\n",
    "target_columns = ['Number of reported results', 'Number in hard mode', '1 try', '2 tries', '3 tries', '4 tries', '5 tries', '6 tries', '7 or more tries (X)']\n",
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        word = row['Word']\n",
    "\n",
    "        # Resize every word into 5 tokens, if word is longer than 5 tokens, truncate it, if word is shorter than 5 tokens, pad it with 0\n",
    "        word = resize_word_list([word])[0]\n",
    "\n",
    "        month = row['Month']\n",
    "        day = row['Day']\n",
    "        weeknum = row['WeekNum']\n",
    "        contest_num = row['Contest number']\n",
    "        targets = row[['Number of reported results', 'Number in hard mode', '1 try', '2 tries', '3 tries', '4 tries', '5 tries', '6 tries', '7 or more tries (X)']].values.astype(float)\n",
    "\n",
    "        # Tokenize the input word\n",
    "        input_ids = self.tokenizer.encode(word, add_special_tokens=True, return_tensors='pt', max_length=5, truncation=True)\n",
    "        # fixed length of 5 tokens\n",
    "        if input_ids.shape[1] < 5:\n",
    "            input_ids = torch.cat((input_ids, torch.zeros((1, 5-input_ids.shape[1])).long()), dim=1)\n",
    "\n",
    "        # attention_mask = torch.ones(5)\n",
    "        attention_mask = torch.ones(input_ids.shape)\n",
    "\n",
    "        # Combine inputs into a dictionary\n",
    "        inputs = {\n",
    "            'input_ids': input_ids.squeeze(0),\n",
    "            'attention_mask': attention_mask.squeeze(0),\n",
    "            'input_numbers': torch.tensor([[month, day, weeknum, contest_num]]).float()\n",
    "        }\n",
    "\n",
    "        # # Print inputs and targets shapes\n",
    "        # print(f\"Input IDs shape: {inputs['input_ids'].shape}\")\n",
    "        # print(f\"Attention mask shape: {inputs['attention_mask'].shape}\")\n",
    "        # print(f\"Input numbers shape: {inputs['input_numbers'].shape}\")\n",
    "        # print(f\"Targets shape: {targets.shape}\")\n",
    "\n",
    "\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to train model\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    epoch_count = 0\n",
    "\n",
    "    with tqdm(total=len(dataloader)) as pbar:\n",
    "        for batch_inputs, batch_targets in dataloader:\n",
    "            # Move batch to device\n",
    "            batch_inputs = {key: value.to(device) for key, value in batch_inputs.items()}\n",
    "            batch_targets = batch_targets.to(device).float()\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            batch_predictions = model(**batch_inputs)\n",
    "            loss = criterion(batch_predictions, batch_targets)\n",
    "\n",
    "            # Backward pass and update weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            epoch_count += 1\n",
    "            pbar.update(1)\n",
    "            pbar.set_description(f\"Batch loss: {loss.item():.4f}\")\n",
    "\n",
    "            if epoch_count % 10 == 0:\n",
    "                torch.save(model.state_dict(), \"checkpoints/\"+\"wordle\"+\"_\"+str(epoch_count)+\"batches\")\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load data and tokenizer\n",
    "data = pd.read_csv('Data_V1.2.csv')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Resize every word into 5 tokens, if word is longer than 5 tokens, truncate it, if word is shorter than 5 tokens, pad it with 0\n",
    "data[word_column] = resize_word_list(data[word_column])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TrainingDataset(data, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Create model and move to device\n",
    "model = CustomNet('roberta-base', num_numbers=4)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/180 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      4\u001b[0m     \u001b[39m# Train for one epoch and get average loss\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, dataloader, optimizer, criterion, device)\n\u001b[1;32m      7\u001b[0m     \u001b[39m# Print the loss for the epoch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[58], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m batch_predictions \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch_inputs)\n\u001b[1;32m     18\u001b[0m loss \u001b[39m=\u001b[39m criterion(batch_predictions, batch_targets)\n\u001b[1;32m     20\u001b[0m \u001b[39m# Backward pass and update weights\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-aio/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/wzx14/Dev_LF/MCM2023/models/CustomNet.py:49\u001b[0m, in \u001b[0;36mCustomNet.forward\u001b[0;34m(self, input_ids, attention_mask, input_numbers)\u001b[0m\n\u001b[1;32m     46\u001b[0m numbers_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumbers_fcn(input_numbers)\n\u001b[1;32m     48\u001b[0m \u001b[39m# Concatenate RoBERTa and number outputs\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m combined \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((pooled_output, numbers_output), dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     51\u001b[0m \u001b[39m# Get combined output\u001b[39;00m\n\u001b[1;32m     52\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombined_fcn(combined)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Train for one epoch and get average loss\n",
    "    train_loss = train(model, dataloader, optimizer, criterion, device)\n",
    "\n",
    "    # Print the loss for the epoch\n",
    "    print(f\"Epoch {epoch+1} loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"checkpoints/\"+\"wordle\"+\"_\"+str(num_epochs)+\"final\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-aio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec23a97d99db22869c50b892264b6cc1a9cb976e0e30caade73d7ae1a16bff91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
